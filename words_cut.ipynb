{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1998_news_words():\n",
    "    words_path = './data/news_1_gram.json'\n",
    "    with open(words_path, 'r', encoding='utf-8') as f:\n",
    "        words = json.load(f)\n",
    "    return words\n",
    "\n",
    "def get_THUOCL_words():\n",
    "    data_path = './data/THUOCL'\n",
    "    # 得到文件列表中的所有文件名\n",
    "    files = os.listdir(data_path)\n",
    "    # 遍历文件名列表，将文件中的内容添加到words字典中\n",
    "    words = {}\n",
    "    for file in files:\n",
    "        with open(os.path.join(data_path, file), 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                if len(line) == 2:\n",
    "                    try:\n",
    "                        words[line[0]] = int(line[1])\n",
    "                    except:\n",
    "                        pass\n",
    "    return words\n",
    "\n",
    "def get_n_grams():\n",
    "    words_path = './data/news_2_gram.json'\n",
    "    with open(words_path, 'r', encoding='utf-8') as f:\n",
    "        words = json.load(f)\n",
    "    return words\n",
    "\n",
    "def get_words_dic(data_name):\n",
    "    if data_name == '1998版新闻':\n",
    "       words = get_1998_news_words()\n",
    "    elif data_name == 'THUOCL清华数据源':\n",
    "       words = get_THUOCL_words()\n",
    "    elif data_name == '词库融合':\n",
    "        # 统一数据规模\n",
    "        words = get_1998_news_words()\n",
    "        THUOCL_words = get_THUOCL_words()\n",
    "        count_1998_news = sum(list(words.values()))\n",
    "        count_THUOCL = sum(list(THUOCL_words.values()))\n",
    "        THUOCL_words = {word: int(count_1998_news * count / count_THUOCL) for word, count in words.items()}\n",
    "        # 扩充数据集\n",
    "        words.update(THUOCL_words)\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信鸽: 220963\n",
      "黄蜂: 118861\n",
      "水母: 78147\n",
      "野猫: 62065\n",
      "宠物狗: 56875\n",
      "母狗: 56087\n",
      "毛毛虫: 48631\n",
      "猎豹: 48451\n",
      "犀牛: 46120\n",
      "萨摩: 84\n"
     ]
    }
   ],
   "source": [
    "data_name = 'THUOCL清华数据源'\n",
    "words_dic = get_words_dic(data_name)\n",
    "# 展示字典的元素\n",
    "q = 10 \n",
    "for word, count in words_dic.items():\n",
    "    if q > 0:\n",
    "        print(f'{word}: {count}')\n",
    "        q -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMM(sentence, words_dic):\n",
    "    max_length = max(len(word) for word in words_dic)\n",
    "    sentence = sentence.strip()\n",
    "    words_length = len(sentence)\n",
    "    cut_word_list = []\n",
    "\n",
    "    while words_length > 0:\n",
    "        max_cut_length = min(max_length, words_length)\n",
    "        sub_sentence = sentence[0: max_cut_length]\n",
    "        while max_cut_length > 0: # 最长匹配\n",
    "            if sub_sentence in words_dic: # 在字典中\n",
    "                cut_word_list.append(sub_sentence)\n",
    "                break\n",
    "            elif max_cut_length == 1: # 不在字典中\n",
    "                cut_word_list.append(sub_sentence)\n",
    "                break\n",
    "            else: # 不在字典中，最大框长度-1\n",
    "                max_cut_length = max_cut_length - 1\n",
    "                sub_sentence = sub_sentence[0:max_cut_length]\n",
    "        sentence = sentence[max_cut_length:]\n",
    "        words_length = words_length - max_cut_length\n",
    "\n",
    "    return cut_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BMM(sentence, words_dic):\n",
    "    max_length = max(len(word) for word in words_dic) # 统计词典中词的最长长度\n",
    "    sentence = sentence.strip()\n",
    "    words_length = len(sentence)\n",
    "    cut_word_list = []\n",
    "\n",
    "    while words_length > 0:\n",
    "        max_cut_length = min(max_length, words_length)\n",
    "        sub_sentence = sentence[-max_cut_length:] # 与FMM不同之处\n",
    "        while max_cut_length > 0:\n",
    "            if sub_sentence in words_dic:\n",
    "                cut_word_list.append(sub_sentence)\n",
    "                break\n",
    "            elif max_cut_length == 1:\n",
    "                cut_word_list.append(sub_sentence)\n",
    "                break\n",
    "            else:\n",
    "                max_cut_length = max_cut_length -1\n",
    "                sub_sentence = sub_sentence[-max_cut_length:]  # 与FMM不同之处\n",
    "        sentence = sentence[0:-max_cut_length] # 与FMM不同之处\n",
    "        words_length = words_length - max_cut_length\n",
    "    cut_word_list.reverse()\n",
    "\n",
    "    return cut_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现双向匹配算法中的切词方法\n",
    "def Bi_MM(sentence, words_dic):\n",
    "    bmm_word_list = BMM(sentence, words_dic)\n",
    "    fmm_word_list = FMM(sentence, words_dic)\n",
    "    if bmm_word_list == fmm_word_list:\n",
    "        return bmm_word_list\n",
    "    else:\n",
    "        bmm_word_list_size = len(bmm_word_list)\n",
    "        fmm_word_list_size = len(fmm_word_list)\n",
    "        if bmm_word_list_size != fmm_word_list_size:\n",
    "            return bmm_word_list if bmm_word_list_size < fmm_word_list_size else fmm_word_list\n",
    "        else:\n",
    "            FMM_one_word_count = sum([1 for word in fmm_word_list if len(word) == 1])\n",
    "            BMM_one_word_count = sum([1 for word in bmm_word_list if len(word) == 1])\n",
    "            return fmm_word_list if BMM_one_word_count > FMM_one_word_count else bmm_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram算法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 通过最大前向匹配算法和最大后向匹配算法得到两个分词结果\n",
    " - 计算每个分词结果的概率值\n",
    " - 返回概率值最高的分词结果\n",
    "\n",
    " 按照自己的理解，中文分词实际上是求解最优解问题（搜索问题），而n-gram模型在问题中扮演的角色是评估函数，而拥有的状态（即分词结果）的越多，越有可能选出最佳的分词结果。\n",
    " 所有分词结果最佳获取的方式是穷举，但是穷举通过n-gram计算的结果离散程度太大，效率很低。\n",
    " 为了能够简化分词结果获取的过程，使用了通过FMM和BMM计算获得两个分词结果。\n",
    " 这两种算法的分词效果较好，并且可以充分利用词库的优势，筛选掉一些低概率的分词结果，用高效的方式接近最优解。同时也可以让我跟专注于n-gram算法的实现以及后续的实验。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_gram_p(begin, end, n_grams):\n",
    "    # 得到分母\n",
    "    all = sum([count for word, count in n_grams.items() if word.split()[0] == begin])\n",
    "    # 得到分子\n",
    "    part = sum([count for word, count in n_grams.items() if word.split()[0] == begin and word.split()[1] == end])\n",
    "    # 数据平滑（加一法） 计算并返回概率值\n",
    "    return (part + 1) / (all + len(n_grams))\n",
    "\n",
    "def get_sentence_p(result, n_grams):\n",
    "    # 使 i = 1 有意义，添加 <BOS> 和 <Eos>\n",
    "    result.insert(0, '<BOS>')\n",
    "    result.append('<EOS>')\n",
    "    # 计算句子概率值\n",
    "    p = 1\n",
    "    for i in range(len(result)-1):\n",
    "        p *= get_2_gram_p(result[i], result[i+1], n_grams)\n",
    "\n",
    "    return p\n",
    "\n",
    "def N_Gram(sentence, words_dic):\n",
    "\n",
    "    n_grams = get_n_grams()\n",
    "\n",
    "    bmm_word_list = BMM(sentence, words_dic)\n",
    "    fmm_word_list = FMM(sentence, words_dic)\n",
    "    if bmm_word_list == fmm_word_list:\n",
    "        return bmm_word_list\n",
    "    else:\n",
    "        # 计算概率值\n",
    "        p_bmm = get_sentence_p(bmm_word_list, n_grams)\n",
    "        p_fmm = get_sentence_p(fmm_word_list, n_grams)\n",
    "        return bmm_word_list if p_bmm > p_fmm else fmm_word_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', '我们', '要', '为', '中国', '人民', '办公', '益', '事业', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "func_dic = {\n",
    "    'FMM': FMM,\n",
    "    'BMM': BMM,\n",
    "    'Bi-MM': Bi_MM,\n",
    "    'N-Gram':N_Gram\n",
    "}\n",
    "\n",
    "data_name = [\n",
    "    '1998版新闻',\n",
    "    'THUOCL清华数据源',\n",
    "    '词库融合'\n",
    "]\n",
    "\n",
    "func = 'N-Gram'\n",
    "data_name = '词库融合'\n",
    "sentence = '我们要为中国人民办公益事业' # 为人民办公益事业 我来到北京清华大学\n",
    "words_dic = get_words_dic(data_name)\n",
    "result = func_dic[func](sentence, words_dic)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
